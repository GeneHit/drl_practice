# PPO + Curiosity + PyTorch DDP Distributed Reinforcement Learning Architecture


## Command
Verify the implementation without DDP.
```
python practice/cli.py --config practice/exercise10_ddp_ppo/config_reacher.py
```
Train (change `pusher` to `standup` if needed)
```bash
torchrun --nproc_per_node=3  --master_addr="127.0.0.1" --master_port=12345 practice/cli.py --config practice/exercise10_ddp_ppo/config_ddp_reacher.py
```

Play with trained model and generate video
```bash
python practice/cli.py --config practice/exercise10_ddp_ppo/config_ddp_reacher.py --mode play
```

Push to hub
```bash
# generate video and push to hub
python practice/cli.py --config practice/exercise10_ddp_ppo/config_ddp_reacher.py --mode push_to_hub --username myuser

# only push to hub
python practice/cli.py --config practice/exercise10_ddp_ppo/config_ddp_reacher.py --mode push_to_hub --username myuser --skip_play
```

Run the comprehensive test suite:
```bash
# Run all tests
python -m pytest practice/exercise10_ddp_ppo/tests/ -v
```


## ğŸ§  Project Goal

Design a scalable onâ€‘policy RL system that integrates:

- âœ… **PPO (Proximal Policy Optimization)** â€” stable, clipped surrogate objective
- âœ… **Curiosity Module** â€” intrinsic reward to drive exploration (ICM / RND / NGU)
- âœ… **PyTorch DDP (DistributedDataParallel)** â€” multiâ€‘GPU synchronous training
- âœ… **Environment Parallelism** â€” vectorized or multiâ€‘process sampling (e.g., Pusher-v5, HumanoidStandup-v5)

### ğŸ“ Architecture Overview

This design decouples sampling from training while leveraging both environment parallelism and multiâ€‘GPU DDP:

1. **Parallel Environments**
   - CPUâ€‘based vectorized or subprocess environments collect trajectories.
2. **Data Sharding**
   - Collected onâ€‘policy trajectories are concatenated into one big batch and split across GPUs.
3. **Curiosity Module**
   - Computes intrinsic reward for each transition (e.g. ICM prediction error or RND error).
   - Intrinsic reward is combined with extrinsic reward before advantage estimation.
4. **Multiâ€‘GPU PPO Update**
   - Wrap policy & value networks with `torch.nn.parallel.DistributedDataParallel`.
   - Each GPU takes its data shard, computes losses, calls `backward()`.
   - DDP automatically allâ€‘reduces gradients and synchronizes parameters.
   - `optimizer.step()` is called on each GPU to update the shared model.

### ğŸ” System Flow Diagram

```text
[DP 0]: Vectorized Envs (CPU) â”€â”€collectâ”€â”€â–¶ Rollout â”€â”€â–¶ forward/loss/backward â”€â”€allâ€‘reduceâ”€â”€â–¶ optimizer.step()
                                                                                   |
                                                                                   |
[DP 1]: Vectorized Envs (CPU) â”€â”€collectâ”€â”€â–¶ Rollout â”€â”€â–¶ forward/loss/backward â”€â”€allâ€‘reduceâ”€â”€â–¶ optimizer.step()
                                                                                   |
                                                                                   |
[DP 2]: Vectorized Envs (CPU) â”€â”€collectâ”€â”€â–¶ Rollout â”€â”€â–¶ forward/loss/backward â”€â”€allâ€‘reduceâ”€â”€â–¶ optimizer.step()
...
[DP n]
```
