# SAC + PER + PyTorch DDP Distributed Reinforcement Learning Architecture

## ğŸ§  Project Goal

Design a scalable distributed reinforcement learning system that integrates:

- âœ… **SAC (Soft Actor-Critic)** â€” stable, off-policy continuous control
- âœ… **PER (Prioritized Experience Replay)** â€” prioritizes valuable experiences by TD-error
- âœ… **PyTorch DDP (DistributedDataParallel)** â€” multi-GPU parallel training
- âœ… Multi-environment sampling (e.g., Pusher-v5, Humanoid-v5)

### ğŸ“ Architecture Overview

This is an Actor-Learner decoupled architecture:

- Each **DDP process** acts as an "Actor + Learner" unit
- All processes share a **centralized PER buffer** or use a **distributed PER server**.
- The model is wrapped with PyTorch DDP for **synchronized parameter updates**
- Each process samples and trains independently using batches from the PER buffer

### ğŸ” System Flow Diagram

```text
[DP 0]: Vectorized Envs (CPU) â”€â”€collectâ”€â”€â–¶ Buffer â”€â”€â–¶ forward/loss/backward â”€â”€allâ€‘reduceâ”€â”€â–¶ optimizer.step()
                                                                                   |
                                                                                   |
[DP 1]: Vectorized Envs (CPU) â”€â”€collectâ”€â”€â–¶ Buffer â”€â”€â–¶ forward/loss/backward â”€â”€allâ€‘reduceâ”€â”€â–¶ optimizer.step()
                                                                                   |
                                                                                   |
[DP 2]: Vectorized Envs (CPU) â”€â”€collectâ”€â”€â–¶ Buffer â”€â”€â–¶ forward/loss/backward â”€â”€allâ€‘reduceâ”€â”€â–¶ optimizer.step()
...
[DP n]
```
